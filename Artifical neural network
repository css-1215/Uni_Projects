function [Yn On Yt Ot pc tc] = ann2122890(ID, N_ep, lr, bp,Nh, u, v, w, cf, gfx,HL)

% - ID is your student ID (e.g. 2216789)
% - N_ep the number of epochs 
% - lr is the learning rate
% - bp = 1, heuristic unscaled, 2 calculus based bp
% - Nh is the number of nodes on the single hidden layer
% - u, v, w the number of nodes on the three hidden layers
% - cf = 1 use total squared error; 2 use cross-entropy
% - gfx = 0 dont show graphics, gfx~=0 show graphics
% - HL = 1, one hidden layer with Nh neurons, 3 = 3 hidden layers with
% u,v,w neurons
% - Yn are the exact training labels/targets, Ot the training predictions
% - Yt are the exact testing  labels/targets, On the testing  predictions
% - pc is the predicted vector for classification
% - tc is the target vector for classification

set(0,'DefaultLineLineWidth', 2);
set(0,'DefaultLineMarkerSize', 10);

% As a professional touch we should test the validity of our input

if ID ~= 2122890 % making it unique to my student number
    error('ID input is wrong')
end
if or(N_ep <= 0, lr <= 0)
  error('N_ep and/or lr are not valid')
end

if ~ismember(bp,[1,2])
  error('back prop choice is not valid')
end

if gfx ~= 0
  % clear previous graphics windows and dock figures
  clf; close all; set(0,'DefaultFigureWindowStyle','docked')
end
if Nh <= 0
  error('Nh choice is not valid')
end
% Making the code specific to my u,v, w numbers
if u~=33
    error('u choice is not valid')
end


if v~=34
    error('v choice is not valid')
end


if w~=37
    error('w choice is not valid')
end
   

if ~ismember(cf,[1,2])
  error('performance index choice is not valid')
end
% Only want two options for the number of hidden layers
if ~ismember(HL,[1,3])
  error('Hidden Layer choice is not valid')
end


% obtain the raw training data

  A = readmatrix('mnist_train_1000.csv');

% convert and NORMALIZE it into training inputs and target outputs
X_train = A(:,2:end)'/255;  % beware - transpose, data is in columns!
N_train = size(X_train,2); % size(X_train,1/2) gives number of rows/columns
Y_train = zeros(10,N_train);
% set up the one-hot encoding - note that we have to increment by 1
for i=1:N_train
  Y_train(1+A(i,1),i) = 1;
end

% set up weights and biases
% For a single hidden layer with Nh nodes
if HL ==1;
W2 = 0.5-rand(784,Nh); b2 = zeros(Nh,1);
W3 = 0.5-rand(Nh, 10); b3 = zeros(10,1);
% For three hidden layers with u,v,w nodes
elseif HL==3;
W2 = 0.5-rand(784,u); b2 = zeros(u,1);
W3 = 0.5-rand(u, v); b3 = zeros(v,1); 
W4 = 0.5-rand(v,w); b4 = zeros(w,1);
W5 = 0.5-rand(w,10);b5 = zeros(10,1);
else
     error('HL has improper value')
end
% set up a sigmoid activation function for the layers
sig2  = @(x) 1./(1+exp(-x));
dsig2 = @(x) exp(-x)./((1+exp(-x)).^2);
if cf == 1;
  sig3  = @(x) 1./(1+exp(-x));
  dsig3 = @(x) exp(-x)./((1+exp(-x)).^2);
elseif cf == 2;
  sig3  = @(x) exp(x)/sum(exp(x));
else
  error('cf has improper value')
end

% we'll calculate the performance index at the end of each epoch
pivec = zeros(1,N_ep);  % row vector

% we'll calculate the sum of the squared errors at the end of each epoch
sqerr = zeros(1,N_ep);  % row vector

% we now train by looping N_ep times through the training set
for epoch = 1:N_ep
  for i = 1:N_train
    % get X_train(:,i) as an input to the network
    a1 = X_train(:,i);
    % forward prop to the next layer, activate it, repeat
    % Forward propagting through one hidden layers
    if HL == 1;
    n2 = W2'*a1 + b2; a2 = sig2(n2);
    n3 = W3'*a2 + b3; a3 = sig3(n3);
    y = a3; % output from three layer network
    % Forward propgating through 3 hidden layers
    elseif HL == 3;
    n2 = W2'*a1 + b2; a2 = sig2(n2);
    n3 = W3'*a2 + b3; a3 = sig2(n3);
    n4 = W4'*a3 + b4; a4 = sig2(n4);
    n5 = W5'*a4 + b5; a5 = sig3(n5);
    y = a5; % output from 5 layer network
    end
    

    % Backpropagation through three layer network
   
    if HL==1;    
    % calculate A, the diagonal matrices of activation derivatives

     A2 = diag(dsig2(n2));
    
     % we calculate the error in this output
    e3 = Y_train(:,i) - y;
   
    if cf == 1;
      A3 = diag(dsig3(n3)); S3 = -2*A3*e3;
    elseif cf == 2;
      S3 = -e3;
    end  
    
   % back prop the error
    if     bp == 1; e2 = W3*e3; S2 = -2*A2*e2;
    elseif bp == 2; S2 = A2*W3*S3;
    end 
  
    % and use a learning rate to update weights and biases
    W3 = W3 - lr * a2*S3'; b3 = b3 - lr * S3;
    W2 = W2 - lr * a1*S2'; b2 = b2 - lr * S2;

     for i=1:N_train
    y = sig3(W3'*sig2(W2'*X_train(:,i) + b2) + b3);
    if cf == 1;
      err = Y_train(:,i) - y;
      % each error is itself a vector - hence the norm ||err||
      pivec(epoch) = pivec(epoch) + norm(err,2)^2;
    elseif cf == 2
      xent = -sum(Y_train(:,i).*log(y));
      pivec(epoch) = pivec(epoch) + xent;
    end   
  end
    
% Backpropagation through 5 layer network
  elseif HL==3;
     % calculate all As, the diagonal matrices of activation derivatives
      A4 = diag(dsig2(n4));
      A3 = diag(dsig2(n3));
      A2 = diag(dsig2(n2));


    % we calculate the error in this output
    e5 = Y_train(:,i) - y;
   
    if cf == 1;
      A5 = diag(dsig3(n5)); S5 = -2*A5*e5;
    elseif cf == 2;
      S5 = -e5;
    end  
    
   % back prop the error
    if     bp == 1; 
        e4 = W5*e5; S4 = -2*A4*e4;
        e3 = W4*e4; S3 = -2*A3*e3;
        e2 = W3*e3; S2 = -2*A2*e2;

    elseif bp == 2; 
        S4 = A4*W5*S5;
        S3 = A3*W4*S4;
        S2 = A2*W3*S3;
    end 
  
    % and use a learning rate to update weights and biases
    W5 = W5 - lr * a4*S5'; b5 = b5 - lr * S5;
    W4 = W4 - lr * a3*S4'; b4 = b4 - lr * S4;
    W3 = W3 - lr * a2*S3'; b3 = b3 - lr * S3;
    W2 = W2 - lr * a1*S2'; b2 = b2 - lr * S2;
     
  
   % calculate the sum of squared errors and store for plotting
  for i=1:N_train
    y = sig3(W5'*sig2(W4'* sig2(W3' * sig2(W2'*X_train(:,i) + b2) + b3)+b4)+b5);
    if cf == 1;
      err = Y_train(:,i) - y;
      % each error is itself a vector - hence the norm ||err||
      pivec(epoch) = pivec(epoch) + norm(err,2)^2;
    elseif cf == 2
      xent = -sum(Y_train(:,i).*log(y));
      pivec(epoch) = pivec(epoch) + xent;
    end   
  end
    end
  end

% add a subplot; plot the performance index vs (row vector) epochs
if gfx ~= 0
  subplot(2,2,1)
  plot([1:N_ep],pivec,'b');
  xlabel('epochs'); ylabel('performance index');
end

% loop through the training set and evaluate accuracy of prediction
wins = 0;
y_pred = zeros(10,N_train);
for i = 1:N_train
    if HL==1; % evaluate only the three layer network's predictions
  y_pred(:,i) = sig3(W3'*sig2(W2'*X_train(:,i) + b2) + b3);
    elseif HL==3; % evaluate only the five layer network's predictions
  y_pred(:,i) = sig3(W5'*sig2(W4'* sig2(W3' * sig2(W2'*X_train(:,i) + b2) + b3)+b4)+b5);
    end
  [~, indx1] = max(y_pred(:,i));
  [~, indx2] = max(Y_train(:,i));
  barcol = 'r';
  if indx1 == indx2; wins = wins+1; barcol = 'b'; end 
  if gfx ~= 0
    % plot the output 10-vector at top right
    subplot(2,2,2); bar(0:9,y_pred(:,i),barcol);
    title('predicted output (approximate one-hot)')
    % plot the MNIST image at bottom left
    B = reshape(1-X_train(:,i),[28,28]);
    subplot(2,2,3); imh = imshow(B','InitialMagnification','fit');
    subplot(2,2,4);
    b = bar(categorical({'Wins','Losses'}), [wins i-wins]);
    ylim([0,N_train]);
    b.FaceColor = 'flat'; b.CData(1,:) = [1 0 0]; b.CData(2,:) = [0 0 1];
    a = get(gca,'XTickLabel'); set(gca,'XTickLabel',a,'fontsize',18)    
    drawnow
  end
end
fprintf('training set wins = %d/%d, %f%%\n',wins,N_train,100*wins/N_train)

% assign outputs
Yn=Y_train; On=y_pred;

% obtain the raw test data
  A = readmatrix('mnist_test_100.csv');

% convert and NORMALIZE it into testing inputs and target outputs
X_test = A(:,2:end)'/255;
% the number of data points
N_test = size(X_test,2);
Y_test = zeros(10,N_test);
% set up the one-hot encoding - recall we have to increment by 1
for i=1:N_test
  Y_test(1+A(i,1),i) = 1;
end

% loop through the test set and evaluate accuracy of prediction

ID =[num2str(ID)-'0']; % turning my ID number into a matrix
wins = 0;

% Set up vectors to record ID classification 
pred_class = zeros(1,N_test);  
test_class = zeros(1,N_test);
% Set up matrix for hand digit classification
y_pred = zeros(10,N_test);

for i = 1:N_test
    if HL==1; % only test the three layer network's output
  y_pred(:,i) = sig3(W3'*sig2(W2'*X_test(:,i) + b2) + b3);
    elseif HL==3; % only test the five layer network's output
  y_pred(:,i) = sig3(W5'*sig2(W4'* sig2(W3' * sig2(W2'*X_train(:,i) + b2) + b3)+b4)+b5);
    end
  [~, indx1] = max(y_pred(:,i));
  [~, indx2] = max(Y_test(:,i));
  barcol = 'r';
  if indx1 == indx2; wins = wins+1; barcol = 'b'; end
 % If the digit is predicted correctly, then it can be classified
 % correctly
 % If its predicted as a member of my ID and the test digit is a member,
 % both take on the value 1. If both say the digit is not a member then
 % both are zero.
  if and(indx1==indx2,ismember(indx1,ID)); pred_class(i) = 1; test_class(i) = 1;end
  if and(indx1==indx2,~ismember(indx1,ID)); pred_class(i) = 0; test_class(i) = 0;end 
  % If the digit is not predicted correctly then it cannot be classfied
  % correctly
  % The predicted output must be the opposite of the test output
  % classification
  if and(indx1~=indx2,ismember(indx2,ID)); pred_class(i) = 0; test_class(i) = 1;end 
  if and(indx1~=indx2,~ismember(indx2,ID)); pred_class(i) = 1; test_class(i) = 0;end 
  
  if gfx ~= 0
    % plot the output 10-vector at top right
    subplot(2,2,2); bar(0:9,y_pred(:,i),barcol);
    title('predicted output (approximate one-hot)')
    % plot the MNIST image at bottom left
    B = reshape(1-X_test(:,i),[28,28]);
    subplot(2,2,3); imh = imshow(B','InitialMagnification','fit');
    % animate the wins and losses bottom right
    subplot(2,2,4);
    b = bar(categorical({'Wins','Losses'}), [wins i-wins]);
    ylim([0,N_test]);
    b.FaceColor = 'flat'; b.CData(1,:) = [1 0 0]; b.CData(2,:) = [0 0 1];
    a = get(gca,'XTickLabel'); set(gca,'XTickLabel',a,'fontsize',18)
    drawnow
    pause(0.01)

   end
end
fprintf('testing  set wins = %d/%d, %f%%\n',wins,N_test,100*wins/N_test)

% assign outputs
Yt=Y_test; Ot=y_pred;
pc = pred_class; tc = test_class;


end